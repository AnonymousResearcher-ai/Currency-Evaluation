{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e4916e-28cf-463c-9d1a-bde09b9a03ae",
   "metadata": {},
   "source": [
    "### Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f145811-3c0f-42d1-a30e-ea8c1d29d6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 images\n",
      "Processed 1000 images\n",
      "Processed 1500 images\n",
      "Processed 2000 images\n",
      "Processed 2500 images\n",
      "Processed 3000 images\n",
      "Processed 3500 images\n",
      "Processed 4000 images\n",
      "Processed 4500 images\n",
      "Processed 5000 images\n",
      "Processed 5500 images\n",
      "Processed 6000 images\n",
      "Processed 6500 images\n",
      "Processed 7000 images\n",
      "Processed 7500 images\n",
      "Processed 8000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 03:28:20,722 - Epoch 1/20: Train Loss = 0.3096, Train Acc = 0.8923, Val Loss = 0.1879, Val Acc = 0.9628\n",
      "2025-01-31 03:28:21,481 - Epoch 2/20: Train Loss = 0.1459, Train Acc = 0.9744, Val Loss = 0.1222, Val Acc = 0.9826\n",
      "2025-01-31 03:28:22,231 - Epoch 3/20: Train Loss = 0.1013, Train Acc = 0.9826, Val Loss = 0.0850, Val Acc = 0.9850\n",
      "2025-01-31 03:28:22,976 - Epoch 4/20: Train Loss = 0.0720, Train Acc = 0.9882, Val Loss = 0.0692, Val Acc = 0.9886\n",
      "2025-01-31 03:28:23,723 - Epoch 5/20: Train Loss = 0.0529, Train Acc = 0.9908, Val Loss = 0.0562, Val Acc = 0.9880\n",
      "2025-01-31 03:28:24,468 - Epoch 6/20: Train Loss = 0.0464, Train Acc = 0.9914, Val Loss = 0.0556, Val Acc = 0.9868\n",
      "2025-01-31 03:28:25,215 - Epoch 7/20: Train Loss = 0.0369, Train Acc = 0.9922, Val Loss = 0.0519, Val Acc = 0.9880\n",
      "2025-01-31 03:28:25,962 - Epoch 8/20: Train Loss = 0.0291, Train Acc = 0.9954, Val Loss = 0.0431, Val Acc = 0.9892\n",
      "2025-01-31 03:28:26,710 - Epoch 9/20: Train Loss = 0.0241, Train Acc = 0.9956, Val Loss = 0.0488, Val Acc = 0.9880\n",
      "2025-01-31 03:28:27,448 - Epoch 10/20: Train Loss = 0.0212, Train Acc = 0.9958, Val Loss = 0.0435, Val Acc = 0.9874\n",
      "2025-01-31 03:28:28,187 - Epoch 11/20: Train Loss = 0.0218, Train Acc = 0.9950, Val Loss = 0.0479, Val Acc = 0.9838\n",
      "2025-01-31 03:28:28,925 - Epoch 12/20: Train Loss = 0.0165, Train Acc = 0.9964, Val Loss = 0.0461, Val Acc = 0.9868\n",
      "2025-01-31 03:28:29,663 - Epoch 13/20: Train Loss = 0.0136, Train Acc = 0.9980, Val Loss = 0.0437, Val Acc = 0.9886\n",
      "2025-01-31 03:28:30,401 - Epoch 14/20: Train Loss = 0.0123, Train Acc = 0.9978, Val Loss = 0.0455, Val Acc = 0.9886\n",
      "2025-01-31 03:28:31,152 - Epoch 15/20: Train Loss = 0.0114, Train Acc = 0.9984, Val Loss = 0.0471, Val Acc = 0.9892\n",
      "2025-01-31 03:28:31,886 - Epoch 16/20: Train Loss = 0.0098, Train Acc = 0.9982, Val Loss = 0.0616, Val Acc = 0.9850\n",
      "2025-01-31 03:28:32,616 - Epoch 17/20: Train Loss = 0.0112, Train Acc = 0.9974, Val Loss = 0.0432, Val Acc = 0.9910\n",
      "2025-01-31 03:28:33,347 - Epoch 18/20: Train Loss = 0.0169, Train Acc = 0.9960, Val Loss = 0.0411, Val Acc = 0.9862\n",
      "2025-01-31 03:28:34,077 - Epoch 19/20: Train Loss = 0.0148, Train Acc = 0.9952, Val Loss = 0.0448, Val Acc = 0.9892\n",
      "2025-01-31 03:28:34,792 - Epoch 20/20: Train Loss = 0.0089, Train Acc = 0.9982, Val Loss = 0.0470, Val Acc = 0.9862\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 03:28:35,013 - Macro-Averaged F1 Score: 0.9854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       935\n",
      "           1       0.98      0.98      0.98       733\n",
      "\n",
      "    accuracy                           0.99      1668\n",
      "   macro avg       0.99      0.99      0.99      1668\n",
      "weighted avg       0.99      0.99      0.99      1668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(10)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Dataset Paths\n",
    "dataset_dir = '/projects/ouzuner/mbiswas2/pubmed/FC/Dataset/modified_dataset'\n",
    "img_size = (299, 299)\n",
    "\n",
    "# Define CustomCNN Model\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x  \n",
    "\n",
    "# Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Feature extraction function\n",
    "def load_images(dataset_dir):\n",
    "    images_list, labels_list, note_paths_list = [], [], []\n",
    "    \n",
    "    counter = 1\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        if class_dir.startswith('.'):\n",
    "            continue\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        \n",
    "        for note_dir in os.listdir(class_path):\n",
    "            if note_dir.startswith('.'):\n",
    "                continue\n",
    "            note_path = os.path.join(class_path, note_dir)\n",
    "            num_images_per_note = len([name for name in os.listdir(note_path) if os.path.isfile(os.path.join(note_path, name))])\n",
    "\n",
    "            for i in range(1, num_images_per_note + 1):\n",
    "                image_path = os.path.join(note_path, f'note_{note_dir.split(\"_\")[1]}_{i}.jpg')\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is None:\n",
    "                    continue\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = transform(image)  # Apply transformation\n",
    "                images_list.append(image)\n",
    "                labels_list.append(0 if class_dir == 'real_notes' else 1)\n",
    "                note_paths_list.append(note_path)\n",
    "\n",
    "                if counter % 500 == 0:\n",
    "                    print(f\"Processed {counter} images\")\n",
    "                counter += 1\n",
    "\n",
    "    X = torch.stack(images_list)\n",
    "    y = torch.tensor(labels_list, dtype=torch.long)\n",
    "    \n",
    "    return X, y, note_paths_list\n",
    "\n",
    "# Load images\n",
    "X, y, note_paths = load_images(dataset_dir)\n",
    "\n",
    "# Train-validation-test split (same as original code)\n",
    "X_train, X_val_test, y_train, y_val_test, note_paths_train, note_paths_val_test = train_test_split(X, y, note_paths, test_size=0.4, random_state=10)\n",
    "X_val, X_test, y_val, y_test, note_paths_val, note_paths_test = train_test_split(X_val_test, y_val_test, note_paths_val_test, test_size=0.5, random_state=10)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CustomCNN().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_customcnn_model.pth')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_customcnn_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Test Model\n",
    "def test_model(model, test_loader):\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt=\"d\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Evaluate model on test data\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d80d0-dec3-4a35-a6af-b9c5c81d3773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8caf0bc-d06a-478f-87da-2ccb46c22cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 286274\n",
      "Trainable parameters: 286274\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your defined PyTorch model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total parameters: {total_params}')\n",
    "print(f'Trainable parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa5428-d321-4bdd-8304-2d02d224dbec",
   "metadata": {},
   "source": [
    "### EfficientNet_B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1b3b3b-d333-4bb9-a669-1640b82a8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 images\n",
      "Processed 1000 images\n",
      "Processed 1500 images\n",
      "Processed 2000 images\n",
      "Processed 2500 images\n",
      "Processed 3000 images\n",
      "Processed 3500 images\n",
      "Processed 4000 images\n",
      "Processed 4500 images\n",
      "Processed 5000 images\n",
      "Processed 5500 images\n",
      "Processed 6000 images\n",
      "Processed 6500 images\n",
      "Processed 7000 images\n",
      "Processed 7500 images\n",
      "Processed 8000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 03:42:29,663 - Epoch 1/25: Train Loss = 0.5943, Train Acc = 0.6936\n",
      "2025-01-31 03:42:33,348 - Epoch 1/25: Val Loss = 0.5828, Val Acc = 0.6685\n",
      "2025-01-31 03:42:46,852 - Epoch 2/25: Train Loss = 0.4031, Train Acc = 0.8241\n",
      "2025-01-31 03:42:47,916 - Epoch 2/25: Val Loss = 0.3113, Val Acc = 0.8753\n",
      "2025-01-31 03:42:59,539 - Epoch 3/25: Train Loss = 0.2752, Train Acc = 0.8869\n",
      "2025-01-31 03:43:01,623 - Epoch 3/25: Val Loss = 0.2058, Val Acc = 0.9293\n",
      "2025-01-31 03:43:17,921 - Epoch 4/25: Train Loss = 0.1807, Train Acc = 0.9317\n",
      "2025-01-31 03:43:18,861 - Epoch 4/25: Val Loss = 0.2430, Val Acc = 0.9155\n",
      "2025-01-31 03:43:29,804 - Epoch 5/25: Train Loss = 0.1388, Train Acc = 0.9462\n",
      "2025-01-31 03:43:30,713 - Epoch 5/25: Val Loss = 0.1591, Val Acc = 0.9460\n",
      "2025-01-31 03:43:43,172 - Epoch 6/25: Train Loss = 0.1187, Train Acc = 0.9560\n",
      "2025-01-31 03:43:44,090 - Epoch 6/25: Val Loss = 0.1395, Val Acc = 0.9520\n",
      "2025-01-31 03:43:54,962 - Epoch 7/25: Train Loss = 0.0916, Train Acc = 0.9664\n",
      "2025-01-31 03:43:55,873 - Epoch 7/25: Val Loss = 0.1400, Val Acc = 0.9622\n",
      "2025-01-31 03:44:14,419 - Epoch 8/25: Train Loss = 0.0754, Train Acc = 0.9698\n",
      "2025-01-31 03:44:15,402 - Epoch 8/25: Val Loss = 0.1284, Val Acc = 0.9622\n",
      "2025-01-31 03:44:26,899 - Epoch 9/25: Train Loss = 0.0735, Train Acc = 0.9756\n",
      "2025-01-31 03:44:27,871 - Epoch 9/25: Val Loss = 0.1300, Val Acc = 0.9616\n",
      "2025-01-31 03:44:38,626 - Epoch 10/25: Train Loss = 0.0653, Train Acc = 0.9776\n",
      "2025-01-31 03:44:39,607 - Epoch 10/25: Val Loss = 0.2112, Val Acc = 0.9317\n",
      "2025-01-31 03:44:50,617 - Epoch 11/25: Train Loss = 0.0678, Train Acc = 0.9768\n",
      "2025-01-31 03:44:53,170 - Epoch 11/25: Val Loss = 0.1369, Val Acc = 0.9538\n",
      "2025-01-31 03:45:05,852 - Epoch 12/25: Train Loss = 0.0743, Train Acc = 0.9740\n",
      "2025-01-31 03:45:06,902 - Epoch 12/25: Val Loss = 0.1241, Val Acc = 0.9634\n",
      "2025-01-31 03:45:18,629 - Epoch 13/25: Train Loss = 0.0498, Train Acc = 0.9802\n",
      "2025-01-31 03:45:19,691 - Epoch 13/25: Val Loss = 0.1888, Val Acc = 0.9418\n",
      "2025-01-31 03:45:30,432 - Epoch 14/25: Train Loss = 0.0570, Train Acc = 0.9782\n",
      "2025-01-31 03:45:31,708 - Epoch 14/25: Val Loss = 0.1173, Val Acc = 0.9694\n",
      "2025-01-31 03:45:44,610 - Epoch 15/25: Train Loss = 0.0429, Train Acc = 0.9862\n",
      "2025-01-31 03:45:45,659 - Epoch 15/25: Val Loss = 0.1242, Val Acc = 0.9700\n",
      "2025-01-31 03:45:56,740 - Epoch 16/25: Train Loss = 0.0455, Train Acc = 0.9838\n",
      "2025-01-31 03:45:58,487 - Epoch 16/25: Val Loss = 0.1047, Val Acc = 0.9772\n",
      "2025-01-31 03:46:13,479 - Epoch 17/25: Train Loss = 0.0285, Train Acc = 0.9906\n",
      "2025-01-31 03:46:14,620 - Epoch 17/25: Val Loss = 0.1088, Val Acc = 0.9766\n",
      "2025-01-31 03:46:27,142 - Epoch 18/25: Train Loss = 0.0407, Train Acc = 0.9870\n",
      "2025-01-31 03:46:28,071 - Epoch 18/25: Val Loss = 0.1175, Val Acc = 0.9724\n",
      "2025-01-31 03:46:38,860 - Epoch 19/25: Train Loss = 0.0356, Train Acc = 0.9884\n",
      "2025-01-31 03:46:39,765 - Epoch 19/25: Val Loss = 0.1387, Val Acc = 0.9688\n",
      "2025-01-31 03:46:51,381 - Epoch 20/25: Train Loss = 0.0319, Train Acc = 0.9886\n",
      "2025-01-31 03:46:53,475 - Epoch 20/25: Val Loss = 0.1211, Val Acc = 0.9724\n",
      "2025-01-31 03:47:04,319 - Epoch 21/25: Train Loss = 0.0396, Train Acc = 0.9864\n",
      "2025-01-31 03:47:05,239 - Epoch 21/25: Val Loss = 0.1406, Val Acc = 0.9634\n",
      "2025-01-31 03:47:16,479 - Epoch 22/25: Train Loss = 0.0323, Train Acc = 0.9890\n",
      "2025-01-31 03:47:17,663 - Epoch 22/25: Val Loss = 0.1156, Val Acc = 0.9742\n",
      "2025-01-31 03:47:29,190 - Epoch 23/25: Train Loss = 0.0283, Train Acc = 0.9916\n",
      "2025-01-31 03:47:30,101 - Epoch 23/25: Val Loss = 0.1212, Val Acc = 0.9688\n",
      "2025-01-31 03:47:42,022 - Epoch 24/25: Train Loss = 0.0253, Train Acc = 0.9924\n",
      "2025-01-31 03:47:42,941 - Epoch 24/25: Val Loss = 0.1283, Val Acc = 0.9718\n",
      "2025-01-31 03:48:09,979 - Epoch 25/25: Train Loss = 0.0361, Train Acc = 0.9894\n",
      "2025-01-31 03:48:10,906 - Epoch 25/25: Val Loss = 0.1114, Val Acc = 0.9706\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 03:48:12,057 - Macro-Averaged F1 Score: 0.9683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       935\n",
      "           1       0.97      0.96      0.96       733\n",
      "\n",
      "    accuracy                           0.97      1668\n",
      "   macro avg       0.97      0.97      0.97      1668\n",
      "weighted avg       0.97      0.97      0.97      1668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Dataset Paths (Updated)\n",
    "dataset_dir = '/projects/ouzuner/mbiswas2/pubmed/FC/Dataset/modified_dataset'\n",
    "img_size = (224, 224)  # EfficientNet standard input size\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "epochs = 25\n",
    "NUM_CLASSES = 2  # Binary classification (Real vs Fake)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load images manually\n",
    "def load_images(dataset_dir):\n",
    "    images_list, labels_list, note_paths_list = [], [], []\n",
    "    counter = 1\n",
    "\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        if class_dir.startswith('.'):\n",
    "            continue\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        \n",
    "        for note_dir in os.listdir(class_path):\n",
    "            if note_dir.startswith('.'):\n",
    "                continue\n",
    "            note_path = os.path.join(class_path, note_dir)\n",
    "            num_images_per_note = len([name for name in os.listdir(note_path) if os.path.isfile(os.path.join(note_path, name))])\n",
    "\n",
    "            for i in range(1, num_images_per_note + 1):\n",
    "                image_path = os.path.join(note_path, f'note_{note_dir.split(\"_\")[1]}_{i}.jpg')\n",
    "                image = datasets.folder.default_loader(image_path)  # Uses PIL for image loading\n",
    "                image = transform(image)  # Apply transformation\n",
    "                images_list.append(image)\n",
    "                labels_list.append(0 if class_dir == 'real_notes' else 1)\n",
    "                note_paths_list.append(note_path)\n",
    "\n",
    "                if counter % 500 == 0:\n",
    "                    print(f\"Processed {counter} images\")\n",
    "                counter += 1\n",
    "\n",
    "    X = torch.stack(images_list)\n",
    "    y = torch.tensor(labels_list, dtype=torch.long)\n",
    "    return X, y, note_paths_list\n",
    "\n",
    "# Load dataset\n",
    "X, y, note_paths = load_images(dataset_dir)\n",
    "\n",
    "# Train-validation-test split (same as original)\n",
    "X_train, X_val_test, y_train, y_val_test, note_paths_train, note_paths_val_test = train_test_split(\n",
    "    X, y, note_paths, test_size=0.4, random_state=10\n",
    ")\n",
    "X_val, X_test, y_val, y_test, note_paths_val, note_paths_test = train_test_split(\n",
    "    X_val_test, y_val_test, note_paths_val_test, test_size=0.5, random_state=10\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(list(zip(X_val, y_val)), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load EfficientNet-B0 Model\n",
    "model = models.efficientnet_b0(weights=None)  # Set weights=\"IMAGENET1K_V1\" for pretrained\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, NUM_CLASSES)  # Adjust output layer\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Function\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}: Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'efficientnet_b0_custom_dataset.pth')\n",
    "\n",
    "# Evaluate Model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt=\"d\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Test model\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c954f819-0932-4064-80d0-5e2fc7f1e9ad",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b240969e-d6d6-4635-8570-42aa11f23372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 images\n",
      "Processed 1000 images\n",
      "Processed 1500 images\n",
      "Processed 2000 images\n",
      "Processed 2500 images\n",
      "Processed 3000 images\n",
      "Processed 3500 images\n",
      "Processed 4000 images\n",
      "Processed 4500 images\n",
      "Processed 5000 images\n",
      "Processed 5500 images\n",
      "Processed 6000 images\n",
      "Processed 6500 images\n",
      "Processed 7000 images\n",
      "Processed 7500 images\n",
      "Processed 8000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 04:00:12,625 - Epoch 1/25: Train Loss = 0.5577, Train Acc = 0.7192\n",
      "2025-01-31 04:00:15,326 - Epoch 1/25: Val Loss = 0.4024, Val Acc = 0.8267\n",
      "2025-01-31 04:00:36,943 - Epoch 2/25: Train Loss = 0.2562, Train Acc = 0.9063\n",
      "2025-01-31 04:00:39,149 - Epoch 2/25: Val Loss = 0.1754, Val Acc = 0.9323\n",
      "2025-01-31 04:00:59,916 - Epoch 3/25: Train Loss = 0.1286, Train Acc = 0.9656\n",
      "2025-01-31 04:01:02,666 - Epoch 3/25: Val Loss = 0.0916, Val Acc = 0.9772\n",
      "2025-01-31 04:01:23,464 - Epoch 4/25: Train Loss = 0.1025, Train Acc = 0.9752\n",
      "2025-01-31 04:01:25,687 - Epoch 4/25: Val Loss = 0.0851, Val Acc = 0.9796\n",
      "2025-01-31 04:01:46,570 - Epoch 5/25: Train Loss = 0.0740, Train Acc = 0.9828\n",
      "2025-01-31 04:01:48,771 - Epoch 5/25: Val Loss = 0.1077, Val Acc = 0.9742\n",
      "2025-01-31 04:02:18,422 - Epoch 6/25: Train Loss = 0.0661, Train Acc = 0.9810\n",
      "2025-01-31 04:02:20,714 - Epoch 6/25: Val Loss = 0.0982, Val Acc = 0.9700\n",
      "2025-01-31 04:02:42,064 - Epoch 7/25: Train Loss = 0.0751, Train Acc = 0.9800\n",
      "2025-01-31 04:02:44,404 - Epoch 7/25: Val Loss = 0.0807, Val Acc = 0.9826\n",
      "2025-01-31 04:03:05,697 - Epoch 8/25: Train Loss = 0.0464, Train Acc = 0.9866\n",
      "2025-01-31 04:03:07,979 - Epoch 8/25: Val Loss = 0.0609, Val Acc = 0.9868\n",
      "2025-01-31 04:03:29,274 - Epoch 9/25: Train Loss = 0.0468, Train Acc = 0.9886\n",
      "2025-01-31 04:03:31,545 - Epoch 9/25: Val Loss = 0.0606, Val Acc = 0.9862\n",
      "2025-01-31 04:04:04,457 - Epoch 10/25: Train Loss = 0.0564, Train Acc = 0.9858\n",
      "2025-01-31 04:04:06,629 - Epoch 10/25: Val Loss = 0.0594, Val Acc = 0.9844\n",
      "2025-01-31 04:04:26,957 - Epoch 11/25: Train Loss = 0.0425, Train Acc = 0.9880\n",
      "2025-01-31 04:04:33,288 - Epoch 11/25: Val Loss = 0.0546, Val Acc = 0.9880\n",
      "2025-01-31 04:04:54,635 - Epoch 12/25: Train Loss = 0.0696, Train Acc = 0.9802\n",
      "2025-01-31 04:04:57,031 - Epoch 12/25: Val Loss = 0.0887, Val Acc = 0.9766\n",
      "2025-01-31 04:05:22,914 - Epoch 13/25: Train Loss = 0.0404, Train Acc = 0.9880\n",
      "2025-01-31 04:05:25,087 - Epoch 13/25: Val Loss = 0.0565, Val Acc = 0.9886\n",
      "2025-01-31 04:05:45,439 - Epoch 14/25: Train Loss = 0.0282, Train Acc = 0.9910\n",
      "2025-01-31 04:05:47,610 - Epoch 14/25: Val Loss = 0.0645, Val Acc = 0.9850\n",
      "2025-01-31 04:06:07,917 - Epoch 15/25: Train Loss = 0.0369, Train Acc = 0.9890\n",
      "2025-01-31 04:06:10,691 - Epoch 15/25: Val Loss = 0.0536, Val Acc = 0.9880\n",
      "2025-01-31 04:06:31,556 - Epoch 16/25: Train Loss = 0.0350, Train Acc = 0.9890\n",
      "2025-01-31 04:06:33,737 - Epoch 16/25: Val Loss = 0.0893, Val Acc = 0.9748\n",
      "2025-01-31 04:06:54,066 - Epoch 17/25: Train Loss = 0.0451, Train Acc = 0.9858\n",
      "2025-01-31 04:06:56,246 - Epoch 17/25: Val Loss = 0.0632, Val Acc = 0.9838\n",
      "2025-01-31 04:07:20,397 - Epoch 18/25: Train Loss = 0.0209, Train Acc = 0.9938\n",
      "2025-01-31 04:07:29,461 - Epoch 18/25: Val Loss = 0.0529, Val Acc = 0.9880\n",
      "2025-01-31 04:07:52,440 - Epoch 19/25: Train Loss = 0.0256, Train Acc = 0.9912\n",
      "2025-01-31 04:07:54,626 - Epoch 19/25: Val Loss = 0.0623, Val Acc = 0.9892\n",
      "2025-01-31 04:08:14,990 - Epoch 20/25: Train Loss = 0.0218, Train Acc = 0.9922\n",
      "2025-01-31 04:08:17,168 - Epoch 20/25: Val Loss = 0.0908, Val Acc = 0.9802\n",
      "2025-01-31 04:08:37,526 - Epoch 21/25: Train Loss = 0.0283, Train Acc = 0.9904\n",
      "2025-01-31 04:08:40,427 - Epoch 21/25: Val Loss = 0.0563, Val Acc = 0.9886\n",
      "2025-01-31 04:09:09,459 - Epoch 22/25: Train Loss = 0.0356, Train Acc = 0.9876\n",
      "2025-01-31 04:09:11,706 - Epoch 22/25: Val Loss = 0.0536, Val Acc = 0.9874\n",
      "2025-01-31 04:09:32,742 - Epoch 23/25: Train Loss = 0.0198, Train Acc = 0.9930\n",
      "2025-01-31 04:09:34,983 - Epoch 23/25: Val Loss = 0.0528, Val Acc = 0.9892\n",
      "2025-01-31 04:09:56,121 - Epoch 24/25: Train Loss = 0.0262, Train Acc = 0.9924\n",
      "2025-01-31 04:09:58,380 - Epoch 24/25: Val Loss = 0.0506, Val Acc = 0.9850\n",
      "2025-01-31 04:10:19,460 - Epoch 25/25: Train Loss = 0.0148, Train Acc = 0.9944\n",
      "2025-01-31 04:10:21,704 - Epoch 25/25: Val Loss = 0.1216, Val Acc = 0.9640\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 04:10:25,339 - Macro-Averaged F1 Score: 0.9526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       935\n",
      "           1       0.99      0.90      0.95       733\n",
      "\n",
      "    accuracy                           0.95      1668\n",
      "   macro avg       0.96      0.95      0.95      1668\n",
      "weighted avg       0.96      0.95      0.95      1668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Dataset Paths\n",
    "dataset_dir = '/projects/ouzuner/mbiswas2/pubmed/FC/Dataset/modified_dataset'\n",
    "img_size = (224, 224)  # VGG-16 standard input size\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "epochs = 25\n",
    "NUM_CLASSES = 2  # Binary classification (Real vs Fake)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load images manually\n",
    "def load_images(dataset_dir):\n",
    "    images_list, labels_list, note_paths_list = [], [], []\n",
    "    counter = 1\n",
    "\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        if class_dir.startswith('.'):\n",
    "            continue\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        \n",
    "        for note_dir in os.listdir(class_path):\n",
    "            if note_dir.startswith('.'):\n",
    "                continue\n",
    "            note_path = os.path.join(class_path, note_dir)\n",
    "            num_images_per_note = len([name for name in os.listdir(note_path) if os.path.isfile(os.path.join(note_path, name))])\n",
    "\n",
    "            for i in range(1, num_images_per_note + 1):\n",
    "                image_path = os.path.join(note_path, f'note_{note_dir.split(\"_\")[1]}_{i}.jpg')\n",
    "                image = datasets.folder.default_loader(image_path)  # Uses PIL for image loading\n",
    "                image = transform(image)  # Apply transformation\n",
    "                images_list.append(image)\n",
    "                labels_list.append(0 if class_dir == 'real_notes' else 1)\n",
    "                note_paths_list.append(note_path)\n",
    "\n",
    "                if counter % 500 == 0:\n",
    "                    print(f\"Processed {counter} images\")\n",
    "                counter += 1\n",
    "\n",
    "    X = torch.stack(images_list)\n",
    "    y = torch.tensor(labels_list, dtype=torch.long)\n",
    "    return X, y, note_paths_list\n",
    "\n",
    "# Load dataset\n",
    "X, y, note_paths = load_images(dataset_dir)\n",
    "\n",
    "# Train-validation-test split (same as original)\n",
    "X_train, X_val_test, y_train, y_val_test, note_paths_train, note_paths_val_test = train_test_split(\n",
    "    X, y, note_paths, test_size=0.4, random_state=10\n",
    ")\n",
    "X_val, X_test, y_val, y_test, note_paths_val, note_paths_test = train_test_split(\n",
    "    X_val_test, y_val_test, note_paths_val_test, test_size=0.5, random_state=10\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(list(zip(X_val, y_val)), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load VGG-16 Model (Replace EfficientNet with VGG-16)\n",
    "model = models.vgg16(weights=None)  # Initialize VGG-16 without pretrained weights\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Function\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}: Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'vgg16_custom_dataset.pth')\n",
    "\n",
    "# Evaluate Model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt=\"d\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Test model\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a700ff-aee8-4b00-8de4-7a45cdb196e1",
   "metadata": {},
   "source": [
    "### ResNet50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7312394-2edd-44bc-b54b-04b48929b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 images\n",
      "Processed 1000 images\n",
      "Processed 1500 images\n",
      "Processed 2000 images\n",
      "Processed 2500 images\n",
      "Processed 3000 images\n",
      "Processed 3500 images\n",
      "Processed 4000 images\n",
      "Processed 4500 images\n",
      "Processed 5000 images\n",
      "Processed 5500 images\n",
      "Processed 6000 images\n",
      "Processed 6500 images\n",
      "Processed 7000 images\n",
      "Processed 7500 images\n",
      "Processed 8000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 04:21:26,336 - Epoch 1/25: Train Loss = 0.5088, Train Acc = 0.7492\n",
      "2025-01-31 04:21:27,717 - Epoch 1/25: Val Loss = 0.3297, Val Acc = 0.8573\n",
      "2025-01-31 04:21:41,460 - Epoch 2/25: Train Loss = 0.2858, Train Acc = 0.8811\n",
      "2025-01-31 04:21:42,837 - Epoch 2/25: Val Loss = 0.4744, Val Acc = 0.8141\n",
      "2025-01-31 04:21:56,687 - Epoch 3/25: Train Loss = 0.2196, Train Acc = 0.9127\n",
      "2025-01-31 04:21:58,082 - Epoch 3/25: Val Loss = 0.2861, Val Acc = 0.8795\n",
      "2025-01-31 04:22:09,657 - Epoch 4/25: Train Loss = 0.1630, Train Acc = 0.9432\n",
      "2025-01-31 04:22:11,026 - Epoch 4/25: Val Loss = 0.5404, Val Acc = 0.8573\n",
      "2025-01-31 04:22:22,587 - Epoch 5/25: Train Loss = 0.1169, Train Acc = 0.9586\n",
      "2025-01-31 04:22:23,954 - Epoch 5/25: Val Loss = 0.1424, Val Acc = 0.9430\n",
      "2025-01-31 04:22:36,190 - Epoch 6/25: Train Loss = 0.1259, Train Acc = 0.9558\n",
      "2025-01-31 04:22:37,573 - Epoch 6/25: Val Loss = 0.2731, Val Acc = 0.9017\n",
      "2025-01-31 04:22:51,487 - Epoch 7/25: Train Loss = 0.1190, Train Acc = 0.9618\n",
      "2025-01-31 04:22:52,871 - Epoch 7/25: Val Loss = 0.3727, Val Acc = 0.8957\n",
      "2025-01-31 04:23:06,258 - Epoch 8/25: Train Loss = 0.0938, Train Acc = 0.9706\n",
      "2025-01-31 04:23:07,687 - Epoch 8/25: Val Loss = 0.1410, Val Acc = 0.9544\n",
      "2025-01-31 04:23:19,504 - Epoch 9/25: Train Loss = 0.0695, Train Acc = 0.9774\n",
      "2025-01-31 04:23:21,232 - Epoch 9/25: Val Loss = 0.0874, Val Acc = 0.9706\n",
      "2025-01-31 04:23:33,618 - Epoch 10/25: Train Loss = 0.0673, Train Acc = 0.9770\n",
      "2025-01-31 04:23:35,012 - Epoch 10/25: Val Loss = 0.0815, Val Acc = 0.9778\n",
      "2025-01-31 04:23:46,753 - Epoch 11/25: Train Loss = 0.0725, Train Acc = 0.9754\n",
      "2025-01-31 04:23:48,706 - Epoch 11/25: Val Loss = 0.0976, Val Acc = 0.9754\n",
      "2025-01-31 04:24:01,549 - Epoch 12/25: Train Loss = 0.0522, Train Acc = 0.9826\n",
      "2025-01-31 04:24:02,978 - Epoch 12/25: Val Loss = 0.0845, Val Acc = 0.9766\n",
      "2025-01-31 04:24:14,727 - Epoch 13/25: Train Loss = 0.0438, Train Acc = 0.9846\n",
      "2025-01-31 04:24:16,149 - Epoch 13/25: Val Loss = 0.1629, Val Acc = 0.9562\n",
      "2025-01-31 04:24:27,898 - Epoch 14/25: Train Loss = 0.0464, Train Acc = 0.9852\n",
      "2025-01-31 04:24:29,283 - Epoch 14/25: Val Loss = 0.1226, Val Acc = 0.9562\n",
      "2025-01-31 04:24:41,090 - Epoch 15/25: Train Loss = 0.0394, Train Acc = 0.9860\n",
      "2025-01-31 04:24:42,487 - Epoch 15/25: Val Loss = 0.0943, Val Acc = 0.9802\n",
      "2025-01-31 04:24:54,280 - Epoch 16/25: Train Loss = 0.0573, Train Acc = 0.9792\n",
      "2025-01-31 04:24:55,690 - Epoch 16/25: Val Loss = 0.1299, Val Acc = 0.9598\n",
      "2025-01-31 04:25:07,445 - Epoch 17/25: Train Loss = 0.0307, Train Acc = 0.9898\n",
      "2025-01-31 04:25:08,831 - Epoch 17/25: Val Loss = 0.0659, Val Acc = 0.9868\n",
      "2025-01-31 04:25:20,645 - Epoch 18/25: Train Loss = 0.0223, Train Acc = 0.9936\n",
      "2025-01-31 04:25:22,010 - Epoch 18/25: Val Loss = 0.1011, Val Acc = 0.9832\n",
      "2025-01-31 04:25:33,771 - Epoch 19/25: Train Loss = 0.0134, Train Acc = 0.9968\n",
      "2025-01-31 04:25:35,309 - Epoch 19/25: Val Loss = 0.0823, Val Acc = 0.9844\n",
      "2025-01-31 04:25:48,336 - Epoch 20/25: Train Loss = 0.0127, Train Acc = 0.9966\n",
      "2025-01-31 04:25:49,751 - Epoch 20/25: Val Loss = 0.1552, Val Acc = 0.9646\n",
      "2025-01-31 04:26:06,962 - Epoch 21/25: Train Loss = 0.0875, Train Acc = 0.9724\n",
      "2025-01-31 04:26:08,299 - Epoch 21/25: Val Loss = 0.0876, Val Acc = 0.9760\n",
      "2025-01-31 04:26:19,928 - Epoch 22/25: Train Loss = 0.0350, Train Acc = 0.9872\n",
      "2025-01-31 04:26:22,613 - Epoch 22/25: Val Loss = 0.0635, Val Acc = 0.9880\n",
      "2025-01-31 04:26:34,885 - Epoch 23/25: Train Loss = 0.0513, Train Acc = 0.9812\n",
      "2025-01-31 04:26:36,233 - Epoch 23/25: Val Loss = 0.0753, Val Acc = 0.9892\n",
      "2025-01-31 04:26:48,377 - Epoch 24/25: Train Loss = 0.0176, Train Acc = 0.9936\n",
      "2025-01-31 04:26:49,721 - Epoch 24/25: Val Loss = 0.0697, Val Acc = 0.9832\n",
      "2025-01-31 04:27:01,842 - Epoch 25/25: Train Loss = 0.0082, Train Acc = 0.9976\n",
      "2025-01-31 04:27:03,194 - Epoch 25/25: Val Loss = 0.0728, Val Acc = 0.9892\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 04:27:04,980 - Macro-Averaged F1 Score: 0.9848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       935\n",
      "           1       0.98      0.98      0.98       733\n",
      "\n",
      "    accuracy                           0.99      1668\n",
      "   macro avg       0.98      0.99      0.98      1668\n",
      "weighted avg       0.99      0.99      0.99      1668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Dataset Paths\n",
    "dataset_dir = '/projects/ouzuner/mbiswas2/pubmed/FC/Dataset/modified_dataset'\n",
    "img_size = (224, 224)  # ResNet standard input size\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "epochs = 25\n",
    "NUM_CLASSES = 2  # Binary classification (Real vs Fake)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load images manually\n",
    "def load_images(dataset_dir):\n",
    "    images_list, labels_list, note_paths_list = [], [], []\n",
    "    counter = 1\n",
    "\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        if class_dir.startswith('.'):\n",
    "            continue\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        \n",
    "        for note_dir in os.listdir(class_path):\n",
    "            if note_dir.startswith('.'):\n",
    "                continue\n",
    "            note_path = os.path.join(class_path, note_dir)\n",
    "            num_images_per_note = len([name for name in os.listdir(note_path) if os.path.isfile(os.path.join(note_path, name))])\n",
    "\n",
    "            for i in range(1, num_images_per_note + 1):\n",
    "                image_path = os.path.join(note_path, f'note_{note_dir.split(\"_\")[1]}_{i}.jpg')\n",
    "                image = datasets.folder.default_loader(image_path)  # Uses PIL for image loading\n",
    "                image = transform(image)  # Apply transformation\n",
    "                images_list.append(image)\n",
    "                labels_list.append(0 if class_dir == 'real_notes' else 1)\n",
    "                note_paths_list.append(note_path)\n",
    "\n",
    "                if counter % 500 == 0:\n",
    "                    print(f\"Processed {counter} images\")\n",
    "                counter += 1\n",
    "\n",
    "    X = torch.stack(images_list)\n",
    "    y = torch.tensor(labels_list, dtype=torch.long)\n",
    "    return X, y, note_paths_list\n",
    "\n",
    "# Load dataset\n",
    "X, y, note_paths = load_images(dataset_dir)\n",
    "\n",
    "# Train-validation-test split (same as original)\n",
    "X_train, X_val_test, y_train, y_val_test, note_paths_train, note_paths_val_test = train_test_split(\n",
    "    X, y, note_paths, test_size=0.4, random_state=10\n",
    ")\n",
    "X_val, X_test, y_val, y_test, note_paths_val, note_paths_test = train_test_split(\n",
    "    X_val_test, y_val_test, note_paths_val_test, test_size=0.5, random_state=10\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(list(zip(X_val, y_val)), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load ResNet-50 Model\n",
    "model = models.resnet50(weights=None)  # Initialize ResNet-50 without pretrained weights\n",
    "\n",
    "# Modify the final fully connected layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Function\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}: Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'resnet50_custom_dataset.pth')\n",
    "\n",
    "# Evaluate Model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt=\"d\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Test model\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648e406-ae1e-427c-bf99-b54e11c034d2",
   "metadata": {},
   "source": [
    "### Inception_V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef652ff-0dd5-4d89-b4d8-b14170424112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 images\n",
      "Processed 1000 images\n",
      "Processed 1500 images\n",
      "Processed 2000 images\n",
      "Processed 2500 images\n",
      "Processed 3000 images\n",
      "Processed 3500 images\n",
      "Processed 4000 images\n",
      "Processed 4500 images\n",
      "Processed 5000 images\n",
      "Processed 5500 images\n",
      "Processed 6000 images\n",
      "Processed 6500 images\n",
      "Processed 7000 images\n",
      "Processed 7500 images\n",
      "Processed 8000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 04:38:47,834 - Epoch 1/25: Train Loss = 0.4489, Train Acc = 0.7938\n",
      "2025-01-31 04:38:49,863 - Epoch 1/25: Val Loss = 0.3214, Val Acc = 0.8657\n",
      "2025-01-31 04:39:10,901 - Epoch 2/25: Train Loss = 0.2015, Train Acc = 0.9253\n",
      "2025-01-31 04:39:12,831 - Epoch 2/25: Val Loss = 0.1616, Val Acc = 0.9466\n",
      "2025-01-31 04:39:32,742 - Epoch 3/25: Train Loss = 0.1180, Train Acc = 0.9608\n",
      "2025-01-31 04:39:34,948 - Epoch 3/25: Val Loss = 0.0932, Val Acc = 0.9694\n",
      "2025-01-31 04:39:56,781 - Epoch 4/25: Train Loss = 0.0897, Train Acc = 0.9710\n",
      "2025-01-31 04:39:58,622 - Epoch 4/25: Val Loss = 0.1498, Val Acc = 0.9454\n",
      "2025-01-31 04:40:17,305 - Epoch 5/25: Train Loss = 0.0743, Train Acc = 0.9770\n",
      "2025-01-31 04:40:20,433 - Epoch 5/25: Val Loss = 0.1120, Val Acc = 0.9724\n",
      "2025-01-31 04:40:39,696 - Epoch 6/25: Train Loss = 0.0528, Train Acc = 0.9824\n",
      "2025-01-31 04:40:41,518 - Epoch 6/25: Val Loss = 0.0650, Val Acc = 0.9850\n",
      "2025-01-31 04:41:00,103 - Epoch 7/25: Train Loss = 0.0510, Train Acc = 0.9824\n",
      "2025-01-31 04:41:02,473 - Epoch 7/25: Val Loss = 0.0713, Val Acc = 0.9802\n",
      "2025-01-31 04:41:21,165 - Epoch 8/25: Train Loss = 0.0396, Train Acc = 0.9868\n",
      "2025-01-31 04:41:23,044 - Epoch 8/25: Val Loss = 0.2313, Val Acc = 0.9293\n",
      "2025-01-31 04:41:45,933 - Epoch 9/25: Train Loss = 0.0482, Train Acc = 0.9848\n",
      "2025-01-31 04:41:47,917 - Epoch 9/25: Val Loss = 0.0859, Val Acc = 0.9808\n",
      "2025-01-31 04:42:07,377 - Epoch 10/25: Train Loss = 0.0305, Train Acc = 0.9888\n",
      "2025-01-31 04:42:09,539 - Epoch 10/25: Val Loss = 0.1177, Val Acc = 0.9724\n",
      "2025-01-31 04:42:29,087 - Epoch 11/25: Train Loss = 0.0320, Train Acc = 0.9890\n",
      "2025-01-31 04:42:31,544 - Epoch 11/25: Val Loss = 0.0727, Val Acc = 0.9838\n",
      "2025-01-31 04:42:55,545 - Epoch 12/25: Train Loss = 0.0472, Train Acc = 0.9834\n",
      "2025-01-31 04:42:59,287 - Epoch 12/25: Val Loss = 0.1678, Val Acc = 0.9556\n",
      "2025-01-31 04:43:19,530 - Epoch 13/25: Train Loss = 0.0438, Train Acc = 0.9860\n",
      "2025-01-31 04:43:21,353 - Epoch 13/25: Val Loss = 0.0515, Val Acc = 0.9892\n",
      "2025-01-31 04:43:42,733 - Epoch 14/25: Train Loss = 0.0274, Train Acc = 0.9900\n",
      "2025-01-31 04:43:44,614 - Epoch 14/25: Val Loss = 0.0739, Val Acc = 0.9766\n",
      "2025-01-31 04:44:03,154 - Epoch 15/25: Train Loss = 0.0424, Train Acc = 0.9848\n",
      "2025-01-31 04:44:04,977 - Epoch 15/25: Val Loss = 0.0749, Val Acc = 0.9832\n",
      "2025-01-31 04:44:23,555 - Epoch 16/25: Train Loss = 0.0328, Train Acc = 0.9884\n",
      "2025-01-31 04:44:26,414 - Epoch 16/25: Val Loss = 0.0647, Val Acc = 0.9844\n",
      "2025-01-31 04:44:49,078 - Epoch 17/25: Train Loss = 0.0171, Train Acc = 0.9940\n",
      "2025-01-31 04:44:50,880 - Epoch 17/25: Val Loss = 0.1326, Val Acc = 0.9508\n",
      "2025-01-31 04:45:09,381 - Epoch 18/25: Train Loss = 0.0263, Train Acc = 0.9916\n",
      "2025-01-31 04:45:11,185 - Epoch 18/25: Val Loss = 0.2420, Val Acc = 0.9203\n",
      "2025-01-31 04:45:29,748 - Epoch 19/25: Train Loss = 0.0166, Train Acc = 0.9946\n",
      "2025-01-31 04:45:31,579 - Epoch 19/25: Val Loss = 0.0690, Val Acc = 0.9886\n",
      "2025-01-31 04:45:53,488 - Epoch 20/25: Train Loss = 0.0098, Train Acc = 0.9966\n",
      "2025-01-31 04:45:56,468 - Epoch 20/25: Val Loss = 0.0618, Val Acc = 0.9928\n",
      "2025-01-31 04:46:15,330 - Epoch 21/25: Train Loss = 0.0096, Train Acc = 0.9972\n",
      "2025-01-31 04:46:17,315 - Epoch 21/25: Val Loss = 0.2173, Val Acc = 0.9454\n",
      "2025-01-31 04:46:36,998 - Epoch 22/25: Train Loss = 0.0418, Train Acc = 0.9864\n",
      "2025-01-31 04:46:40,594 - Epoch 22/25: Val Loss = 0.0697, Val Acc = 0.9874\n",
      "2025-01-31 04:47:03,575 - Epoch 23/25: Train Loss = 0.0216, Train Acc = 0.9924\n",
      "2025-01-31 04:47:05,825 - Epoch 23/25: Val Loss = 0.0656, Val Acc = 0.9862\n",
      "2025-01-31 04:47:24,471 - Epoch 24/25: Train Loss = 0.0127, Train Acc = 0.9968\n",
      "2025-01-31 04:47:26,260 - Epoch 24/25: Val Loss = 0.1006, Val Acc = 0.9718\n",
      "2025-01-31 04:47:46,290 - Epoch 25/25: Train Loss = 0.0234, Train Acc = 0.9910\n",
      "2025-01-31 04:47:48,088 - Epoch 25/25: Val Loss = 0.0618, Val Acc = 0.9856\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEWCAYAAABG030jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAaMklEQVR4nO3deZyVdd3/8dd7QAUF2VRU1MSfiKndmFsuye2SCi433NzuleZNUYa7FVk+7NZfd2nmmmahqLjikqW5G2ouubCkJqJJuMCIuAFuoCyf+4/zHRzGmTNnhjnnzHd4P32cB+daznU+Q/b2O5/re12XIgIzM8tHTbULMDOzlnFwm5llxsFtZpYZB7eZWWYc3GZmmXFwm5llxsFtK01SV0l/lrRA0i0rcZyvS7q/LWurBkn3SDq62nVYx+XgXoVIOlLSZEkfSpqTAuarbXDog4G+QJ+IOKS1B4mI6yNi3zaoZwWS9pAUkv7YYP2gtP7hEo/zP5Kua26/iBgaEeNbWa5ZsxzcqwhJpwAXAr+gELKbAL8FhrXB4b8A/DMilrTBscrlbWAXSX3qrTsa+GdbfYEK/P8pKzv/S7YKkNQDOAsYHRG3RcRHEbE4Iv4cET9M+6wh6UJJb6TXhZLWSNv2kDRb0qmS3kqj9WPStjOBM4DD0kh+ZMORqaRN08i2c1r+lqSZkj6Q9Iqkr9db/1i9z+0qaVJqwUyStGu9bQ9L+v+SHk/HuV/SOkX+Gj4F/gQcnj7fCTgMuL7B39VFkmZJel/SFEm7p/VDgJ/U+zmfrVfH/0p6HPgY2Cyt+3bafpmkP9Q7/jmSJkpSqf/7mTXk4F417AJ0Af5YZJ+fAjsD2wKDgJ2A0+ttXx/oAfQDRgKXSuoVET+jMIq/KSK6RcS4YoVIWgu4GBgaEd2BXYFnGtmvN3BX2rcPcD5wV4MR85HAMcB6wOrAD4p9N3ANcFR6vx/wPPBGg30mUfg76A3cANwiqUtE3Nvg5xxU7zPfBEYB3YHXGhzvVOBL6T9Ku1P4uzs6fK8JWwkO7lVDH+CdZloZXwfOioi3IuJt4EwKgVRncdq+OCLuBj4EBraynmXANpK6RsSciJjWyD4HAC9HxLURsSQibgReBA6qt89VEfHPiFgI3EwhcJsUEX8DeksaSCHAr2lkn+si4t30necBa9D8z3l1RExLn1nc4HgfU/h7PB+4Djg+ImY3czyzohzcq4Z3gXXqWhVN2JAVR4uvpXXLj9Eg+D8GurW0kIj4iEKL4nvAHEl3SdqyhHrqaupXb/nNVtRzLXAcsCeN/AYi6QeSpqf2zHwKv2UUa8EAzCq2MSKeAmYCovAfGLOV4uBeNTwBfAIML7LPGxROMtbZhM+3EUr1EbBmveX162+MiPsiYh9gAwqj6MtLqKeuptpW1lTnWuD7wN1pNLxcamX8CDgU6BURPYEFFAIXoKn2RtG2h6TRFEbub6Tjm60UB/cqICIWUDiBeKmk4ZLWlLSapKGSfpV2uxE4XdK66STfGRR+tW+NZ4DBkjZJJ0ZPq9sgqa+kYanX/QmFlsuyRo5xN7BFmsLYWdJhwFbAna2sCYCIeAX4dwo9/Ya6A0sozEDpLOkMYO162+cCm7Zk5oikLYCfA9+g0DL5kaRtW1e9WYGDexWR+rWnUDjh+DaFX++PozDTAgrhMhl4DvgHMDWta813PQDclI41hRXDtibV8QbwHoUQPbaRY7wLHEjh5N67FEaqB0bEO62pqcGxH4uIxn6buA+4l8IUwdeARazYBqm7uOhdSVOb+57UmroOOCcino2IlynMTLm2bsaOWWvIJ7fNzPLiEbeZWWYc3GZmmXFwm5llxsFtZpaZYhdkVFXXLx/ns6b2OfMmXVLtEqwd6tKZlb73S0syZ+HfL6nqvWbabXCbmVVURjd2dHCbmQFkdMNGB7eZGXjEbWaWHY+4zcwyU9Op2hWUzMFtZgZulZiZZcetEjOzzHjEbWaWGY+4zcwy4xG3mVlmPKvEzCwzHnGbmWWmxj1uM7O8eMRtZpYZzyoxM8uMT06amWXGrRIzs8y4VWJmlhmPuM3MMuMRt5lZZjziNjPLjGeVmJllxiNuM7PMuMdtZpaZjEbc+VRqZlZOUumvZg+lkyVNk/S8pBsldZHUX9JTkmZIuknS6mnfNdLyjLR90+aO7+A2M4PCiLvUV7HDSP2AE4AdImIboBNwOHAOcEFEbA7MA0amj4wE5qX1F6T9inJwm5kBqqkp+VWCzkBXSZ2BNYE5wF7ArWn7eGB4ej8sLZO27y0VH9Y7uM3MAEkteY2SNLnea1TdcSKiFvg18DqFwF4ATAHmR8SStNtsoF963w+YlT67JO3fp1itPjlpZgbQgkklETEWGNvoYaReFEbR/YH5wC3AkJWurx6PuM3MaNmIuxlfA16JiLcjYjFwG7Ab0DO1TgA2AmrT+1pg41RDZ6AH8G6xL3Bwm5nRpsH9OrCzpDVTr3pv4AXgIeDgtM/RwO3p/R1pmbT9wYiIYl/gVomZGVBT2knHZkXEU5JuBaYCS4C/U2ir3AVMkPTztG5c+sg44FpJM4D3KMxAKcrBbWYGLepxNycifgb8rMHqmcBOjey7CDikJcd3cJuZQSktkHbDwW1mhoPbzCw7Dm4zs8w4uM3MMqMaB7eZWVY84jYzy4yD28wsN/nktoPbzAw84jYzy46D28wsM211r5JKcHCbmYF73GZmuXGrxMwsMw5uM7PMOLjNzDKT0yXv+ZxG7cBGH7EHk2/5CVNu/SnHHbkHAL84aTjP3HY6T990Gjed9x16dOsKQO8ea3Hv2BN4+/HzuGBMi+69bhk74/TT2GP3XRgx7MDPbRt/9ZUM2nog8+a9V4XKOo42fHRZ2Tm4q2yr/7cBx4zYld2/eS47HfZLhg7ehs02XoeJT77I9of8gp0O+yUvv/YWP/zvfQFY9MlizvrtnZx2wR+rXLlV0rDhI7js91d8bv2bc+bwxOOPs8EGG1ahqo7FwW0l27L/+kx6/lUWLlrM0qXLeHTKDIbvtS0Tn3yRpUuXAfD0P16hX9+eAHy86FP+9sxMFn2yuIpVW6Vtv8OOrN2jx+fWn3vOLzn51B+2izDJXU7BXbYet6QtgWFAv7SqFrgjIqaX6ztzNO1fb/A/xx1E7x5rsfCTTxny1a2Z+sLrK+xz1LBduPX+qVWq0Nqrhx78C+v1XY+BW25Z7VI6hurnccnKEtySxgBHABOAp9PqjYAbJU2IiLOb+NwoYBRA5432oPM6W5ejvHblpVfmct7VD/Dn347m40Wf8uxLs5ePtAF+NHI/li5dxoS7J1WxSmtvFi5cyBVjf8/vLr+y2qV0GO1hJF2qco24RwJbR8QKv89LOh+YBjQa3BExlsJj7On65eOiTLW1O+P/9ATj//QEAGcedxC1c+cD8I2DvsL+g7dh6HcvrmJ11h7NnvU6tbWzOXTEMADmzn2Tww8ewfUTbmGdddetcnV5qsloVkm5gnsZsCHwWoP1G6RtVs+6vbrx9rwP2Xj9XgzbaxD/ftR57LPrFznlW19j329fxMJF7mfbigZsMZCHH31i+fLQffbihptvpVev3lWsKm8eccNJwERJLwOz0rpNgM2B48r0ndm68dffpnfPtVi8ZCknnX0zCz5cyAVjDmWN1Ttz52WFv66n//EqJ/zvBABevOtMuq/VhdVX68xBe/4bB37/Ul6c+WY1fwQrszE/OIXJk55m/vx57LPXYI4dfTwj/svTQdtSRrmNIsrTkZBUA+zEiicnJ0XE0lI+vyq1Sqx08yZdUu0SrB3q0nnlTy0OHHNfyZnz0jn7VTXmyzarJCKWAU+W6/hmZm0ppxG3L3k3M8MnJ83MsuPgNjPLjFslZmaZ8XRAM7PMOLjNzDKTUW47uM3MwCcnzcyy41aJmVlmMsptB7eZGXjEbWaWnYxy28FtZgYecZuZZSenWSV+WLCZGYVWSamv5o+lnpJulfSipOmSdpHUW9IDkl5Of/ZK+0rSxZJmSHpO0nbNHd/BbWZGmz/l/SLg3ojYEhgETAd+DEyMiAHAxLQMMBQYkF6jgMuaO7iD28yMthtxS+oBDAbGAUTEpxExHxgGjE+7jQeGp/fDgGui4Emgp6QNin2Hg9vMjJaNuCWNkjS53mtUvUP1B94GrpL0d0lXSFoL6BsRc9I+bwJ90/t+fPaIR4DZfPbksEb55KSZGS2bVRIRY4GxTWzuDGwHHB8RT0m6iM/aInWfD0mtfjyjR9xmZhRmlZT6asZsYHZEPJWWb6UQ5HPrWiDpz7fS9lpg43qf3yita7rWFv5sZmYdUlv1uCPiTWCWpIFp1d7AC8AdwNFp3dHA7en9HcBRaXbJzsCCei2VRrlVYmZGm1+AczxwvaTVgZnAMRQGyjdLGgm8Bhya9r0b2B+YAXyc9i3KwW1mRtte8h4RzwA7NLJp70b2DWB0S47v4DYzA2p8ybuZWV5yuuTdwW1mBmSU2w5uMzPw3QHNzLKTUW47uM3MAEQ+ye3gNjPDPW4zs+x4VomZWWY8j9vMLDMZ5baD28wMPB3QzCw7GeW2g9vMDKBTRsnt4DYzo4O0SiT9Bmjy0ToRcUJZKjIzq4KMZgMWHXFPrlgVZmZV1iFG3BExvqltZmYdTUa53XyPW9K6wBhgK6BL3fqI2KuMdZmZVVROI+5SHhZ8PTAd6A+cCbwKTCpjTWZmFdepRiW/qq2U4O4TEeOAxRHx14j4b8CjbTPrUNSCV7WVMh1wcfpzjqQDgDeA3uUrycys8jravUp+LqkHcCrwG2Bt4OSyVmVmVmEZ5XbzwR0Rd6a3C4A9y1uOmVl15HRyspRZJVfRyIU4qddtZtYhZJTbJbVK7qz3vgvwnxT63GZmHUZ7mC1SqlJaJX+ovyzpRuCxslVkZlYFHapV0ogBwHptXUhD8yZdUu6vsAwNOPH2apdg7dCsS4et9DFKmRvdXpTS4/6AFXvcb1K4ktLMrMPoUCPuiOheiULMzKopoxZ3878dSJpYyjozs5zldMl7sftxdwHWBNaR1IvPrvRcG+hXgdrMzCqmHeRxyYq1Sr4LnARsCEzhs+B+H/CZQzPrUDJqcRe9H/dFwEWSjo+I31SwJjOzisvpXiWlzIBZJqln3YKkXpK+X76SzMwqr6YFr2orpYbvRMT8uoWImAd8p2wVmZlVgVT6q9pKuQCnkyRFRABI6gSsXt6yzMwqqz3MFilVKcF9L3CTpN+n5e8C95SvJDOzyssot0sK7jHAKOB7afk5YP2yVWRmVgUd6uRkRCwDnqLwrMmdKDy2bHp5yzIzq6wO0eOWtAVwRHq9A9wEEBF+mIKZdTht3SpJ5wMnA7URcaCk/sAEoA+Fa2O+GRGfSloDuAbYHngXOCwiXi1aa5FtL1IYXR8YEV9Nc7mXrvRPY2bWDqkF/5ToRFbsTpwDXBARmwPzgJFp/UhgXlp/QdqvqGLBPQKYAzwk6XJJe9M+HnBsZtbmOteU/mqOpI2AA4Ar0rIoDIRvTbuMB4an98PSMmn73mrmVoVNlhARf4qIw4EtgYcoXP6+nqTLJO3bfOlmZvmQ1JLXKEmT671GNTjchcCPgGVpuQ8wPyKWpOXZfHbPp37ALIC0fUHav0ml3Nb1I+AG4IZ0s6lDKMw0ub+5z5qZ5aIlPe6IGAuMbWybpAOBtyJiiqQ92qK2hlr0BJx01WSTBZuZ5aoNZ4vsBvyHpP0pPKd3beAioKekzmlUvRFQm/avBTYGZkvqDPSgcJKySe3hsnszs6qrkUp+FRMRp0XERhGxKXA48GBEfJ1Cy/ngtNvRQN1z+O5Iy6TtD9Zdqd6U1jxz0sysw+lU/mHsGGCCpJ8DfwfGpfXjgGslzQDeoxD2RTm4zcyAmjJMmouIh4GH0/uZFC5ibLjPIgrnDkvm4DYzo31cEVkqB7eZGR3vJlNmZh1eTjeZcnCbmeFWiZlZdjragxTMzDq8nC5qcXCbmVG4V0kuHNxmZuR161MHt5kZnlViZpadfGLbwW1mBkCNZ5WYmeXFs0rMzDLjWSVmZpnJJ7Yd3GZmgEfcZmbZ6eTgNjPLSz6x7eA2MwN8d0Azs+yU49Fl5eLgNjPDI24zs+zII24zs7x4VomZWWYyym0Ht5kZOLjNzLLjHreZWWYyuqurg9vMDPwEHDOz7LhVYq12xumn8chfH6Z37z7cdvudAPzw1JN47ZVXAPjggw/o3r07N992ezXLtDLbbL1u/HbkDsuXN+mzJufd9SJvzl/EyQcMZEDf7hx07iM89/r85ftsueHanH3EILp17UwsgwN/9Vc+WbKsCtXnya0Sa7Vhw0dwxJHf4KenjVm+7tzzLlz+/te/Optu3bpVoTKrpJlvfciQXz4MFAJl0i/2495n59B1tU6MGjuJs48YtML+nWrExd/ajhPHT2V67fv0XGs1Fi91aLeER9zWatvvsCO1tbMb3RYR3H/fPVx+5fgKV2XV9NWB6/La2x9R+97CJvcZ/MV1mV77PtNr3wdg/keLK1Veh5FRi9vBnZOpUybTp08fvvCFTatdilXQf+zQj9un1BbdZ7P1uhHAdaN3oXe31bljSi2/+8uMyhTYQWSU25V/PqakY4psGyVpsqTJ4y4fW8mysnDP3XcyZP8Dq12GVdBqncQ+X1qfu6a+UXS/zjVix816c/zVUxhx/mMMGbQBuw1cp0JVdgydpJJf1VaNEfeZwFWNbYiIscBYgEVLiEoW1d4tWbKEiX95gAk331btUqyC9ty6L8/PWsA7H3xSdL858xfx1Ix3mffRpwA8NG0u22zck8dfeqcSZXYM1c/jkpUluCU919QmoG85vrOje+qJv9G//2b0XX/9apdiFTRs+37cPrl4mwTgry+8xfe+tjldVuvE4qXL+MqAdbjiwX9VoMKOwycnC+G8HzCvwXoBfyvTd3YIY35wCpMnPc38+fPYZ6/BHDv6eEb81yHce8/dDNn/gGqXZxXUdfVO7L7levz4xmeXrxsyaAPOOuRL9O62Olcf+xVemP0+37j0CRYsXMzlD/6LO8cMhoAHp83lwWlzq1h9ftpBB6Rkimj7joSkccBVEfFYI9tuiIgjmzuGWyXWmAEnev66fd6sS4etdOxOmrmg5MzZcbMeVY35soy4I2JkkW3NhraZWcVlNOL2dEAzM/K6V0nFpwOambVHasGr6HGkjSU9JOkFSdMknZjW95b0gKSX05+90npJuljSDEnPSdquuVod3GZm0HbJDUuAUyNiK2BnYLSkrYAfAxMjYgAwMS0DDAUGpNco4LLmvsDBbWZGYTpgqf8UExFzImJqev8BMB3oBwwD6u5XMR4Ynt4PA66JgieBnpI2KPYdDm4zMwrTAUt/fXaVd3qNavyY2hT4MvAU0Dci5qRNb/LZNS39gFn1PjY7rWuST06amdGyedz1r/Ju+njqBvwBOCki3le9L4iIkNTqKc8ecZuZ0XatEgBJq1EI7esjou4+FXPrWiDpz7fS+lpg43of3yita5KD28yMlrVKih9HAsYB0yPi/Hqb7gCOTu+PBm6vt/6oNLtkZ2BBvZZKo9wqMTOjTa+/2Q34JvAPSc+kdT8BzgZuljQSeA04NG27G9gfmAF8DDR5B9U6Dm4zM2iz5E63+mjqaHs3sn8Ao1vyHQ5uMzN8d0Azs+z4YcFmZrlxcJuZ5cWtEjOzzGR0c0AHt5kZZNUpcXCbmQFZJbeD28yMvB6k4OA2MyOrAbeD28wMyCq5HdxmZng6oJlZdjJqcTu4zczAwW1mlh23SszMMuMRt5lZZjLKbQe3mRl4xG1mlqF8ktvBbWaGH6RgZpYdt0rMzDLj6YBmZrnJJ7cd3GZmkFVuO7jNzMA9bjOz7Cij5HZwm5nhVomZWXYyGnA7uM3MwNMBzcyy4xG3mVlmHNxmZplxq8TMLDMecZuZZSaj3HZwm5kBWSW3g9vMDPe4zcyy4wcpmJnlxsFtZpYXt0rMzDKT03RARUS1a7BmSBoVEWOrXYe1L/73YtVVU+0CrCSjql2AtUv+92IV5eA2M8uMg9vMLDMO7jy4j2mN8b8XqyifnDQzy4xH3GZmmXFwm5llxsHdzkkaIuklSTMk/bja9Vj1SbpS0luSnq92LVYdDu52TFIn4FJgKLAVcISkrapblbUDVwNDql2EVY+Du33bCZgRETMj4lNgAjCsyjVZlUXEI8B71a7DqsfB3b71A2bVW56d1pnZKszBbWaWGQd3+1YLbFxveaO0zsxWYQ7u9m0SMEBSf0mrA4cDd1S5JjOrMgd3OxYRS4DjgPuA6cDNETGtulVZtUm6EXgCGChptqSR1a7JKsuXvJuZZcYjbjOzzDi4zcwy4+A2M8uMg9vMLDMObjOzzDi4rSwkLZX0jKTnJd0iac2VONbVkg5O768odqMtSXtI2rUV3/GqpHVaW6NZJTm4rVwWRsS2EbEN8CnwvfobJXVuzUEj4tsR8UKRXfYAWhzcZjlxcFslPApsnkbDj0q6A3hBUidJ50qaJOk5Sd8FUMEl6T7kfwHWqzuQpIcl7ZDeD5E0VdKzkiZK2pTCfyBOTqP93SWtK+kP6TsmSdotfbaPpPslTZN0BaAK/52YtVqrRj1mpUoj66HAvWnVdsA2EfGKpFHAgojYUdIawOOS7ge+DAykcA/yvsALwJUNjrsucDkwOB2rd0S8J+l3wIcR8eu03w3ABRHxmKRNKFyF+kXgZ8BjEXGWpAMAX31o2XBwW7l0lfRMev8oMI5CC+PpiHglrd8X+Le6/jXQAxgADAZujIilwBuSHmzk+DsDj9QdKyKauj/114CtpOUD6rUldUvfMSJ99i5J81r3Y5pVnoPbymVhRGxbf0UKz4/qrwKOj4j7Guy3fxvWUQPsHBGLGqnFLEvucVs13QccK2k1AElbSFoLeAQ4LPXANwD2bOSzTwKDJfVPn+2d1n8AdK+33/3A8XULkrZNbx8BjkzrhgK92uqHMis3B7dV0xUU+tdT04Nvf0/ht8A/Ai+nbddQuBPeCiLibWAUcJukZ4Gb0qY/A/9Zd3ISOAHYIZ38fIHPZrecSSH4p1Fombxepp/RrM357oBmZpnxiNvMLDMObjOzzDi4zcwy4+A2M8uMg9vMLDMObjOzzDi4zcwy83/jjqRQmj1PngAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 04:47:50,331 - Macro-Averaged F1 Score: 0.9811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       935\n",
      "           1       0.98      0.98      0.98       733\n",
      "\n",
      "    accuracy                           0.98      1668\n",
      "   macro avg       0.98      0.98      0.98      1668\n",
      "weighted avg       0.98      0.98      0.98      1668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Dataset Paths\n",
    "dataset_dir = '/projects/ouzuner/mbiswas2/pubmed/FC/Dataset/modified_dataset'\n",
    "img_size = (299, 299)  # InceptionV3 requires 299x299 input\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "epochs = 25\n",
    "NUM_CLASSES = 2  # Binary classification (Real vs Fake)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load images manually\n",
    "def load_images(dataset_dir):\n",
    "    images_list, labels_list, note_paths_list = [], [], []\n",
    "    counter = 1\n",
    "\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        if class_dir.startswith('.'):\n",
    "            continue\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        \n",
    "        for note_dir in os.listdir(class_path):\n",
    "            if note_dir.startswith('.'):\n",
    "                continue\n",
    "            note_path = os.path.join(class_path, note_dir)\n",
    "            num_images_per_note = len([name for name in os.listdir(note_path) if os.path.isfile(os.path.join(note_path, name))])\n",
    "\n",
    "            for i in range(1, num_images_per_note + 1):\n",
    "                image_path = os.path.join(note_path, f'note_{note_dir.split(\"_\")[1]}_{i}.jpg')\n",
    "                image = datasets.folder.default_loader(image_path)  # Uses PIL for image loading\n",
    "                image = transform(image)  # Apply transformation\n",
    "                images_list.append(image)\n",
    "                labels_list.append(0 if class_dir == 'real_notes' else 1)\n",
    "                note_paths_list.append(note_path)\n",
    "\n",
    "                if counter % 500 == 0:\n",
    "                    print(f\"Processed {counter} images\")\n",
    "                counter += 1\n",
    "\n",
    "    X = torch.stack(images_list)\n",
    "    y = torch.tensor(labels_list, dtype=torch.long)\n",
    "    return X, y, note_paths_list\n",
    "\n",
    "# Load dataset\n",
    "X, y, note_paths = load_images(dataset_dir)\n",
    "\n",
    "# Train-validation-test split (same as original)\n",
    "X_train, X_val_test, y_train, y_val_test, note_paths_train, note_paths_val_test = train_test_split(\n",
    "    X, y, note_paths, test_size=0.4, random_state=10\n",
    ")\n",
    "X_val, X_test, y_val, y_test, note_paths_val, note_paths_test = train_test_split(\n",
    "    X_val_test, y_val_test, note_paths_val_test, test_size=0.5, random_state=10\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(list(zip(X_val, y_val)), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load InceptionV3 Model\n",
    "model = models.inception_v3(weights=None, aux_logits=False, init_weights=True)  # Initialize InceptionV3 without pretrained weights\n",
    "\n",
    "# Modify the final fully connected layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Function\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}: Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'inception_v3_custom_dataset.pth')\n",
    "\n",
    "# Evaluate Model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt=\"d\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Test model\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4807d00-3637-4351-8b87-ff0bb8130dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
